{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理+特征工程\n",
    "train_x.shape = (1381, 1246)\n",
    "test_x.shape  = (150, 1246)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from scipy import sparse\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('jinnan_round1_train_20181227.csv', encoding = 'gb18030')\n",
    "test  = pd.read_csv('jinnan_round1_testA_20181227.csv', encoding = 'gb18030')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = []\n",
    "for col in train.columns:\n",
    "    stats.append((col, train[col].nunique(), train[col].isnull().sum() * 100 / train.shape[0], train[col].value_counts(normalize=True, dropna=False).values[0] * 100, train[col].dtype))\n",
    "    \n",
    "stats_df = pd.DataFrame(stats, columns=['Feature', 'Unique_values', 'Percentage of missing values', 'Percentage of values in the biggest category', 'type'])\n",
    "stats_df.sort_values('Percentage of missing values', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stats = []\n",
    "for col in test.columns:\n",
    "    stats.append((col, test[col].nunique(), test[col].isnull().sum() * 100 / test.shape[0], test[col].value_counts(normalize=True, dropna=False).values[0] * 100, test[col].dtype))\n",
    "    \n",
    "stats_df = pd.DataFrame(stats, columns=['Feature', 'Unique_values', 'Percentage of missing values', 'Percentage of values in the biggest category', 'type'])\n",
    "stats_df.sort_values('Percentage of missing values', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"收率\"\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(range(train.shape[0]), np.sort(train[target_col].values))\n",
    "plt.xlabel('index', fontsize=12)\n",
    "plt.ylabel('yield', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.distplot(train[target_col].values, bins=50, kde=False, color=\"red\")\n",
    "train_y = pd.DataFrame(train[target_col].values)\n",
    "plt.title(\"Histogram of yield\")\n",
    "plt.xlabel('yield', fontsize=12)\n",
    "plt.show()\n",
    "train_y = train[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除类别唯一的特征\n",
    "for df in [train, test]:\n",
    "    df.drop(['B3', 'B13', 'A13', 'A18', 'A23'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除某一类别占比超过90%的列\n",
    "good_cols = list(train.columns)\n",
    "for col in train.columns:\n",
    "    rate = train[col].value_counts(normalize=True, dropna=False).values[0]\n",
    "    if rate > 0.9:\n",
    "        good_cols.remove(col)\n",
    "        print(col,rate)\n",
    "# 暂时不删除，后面构造特征需要\n",
    "good_cols.append('A1')\n",
    "good_cols.append('A3')\n",
    "good_cols.append('A4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除异常值\n",
    "train = train[train['收率']>0.87]\n",
    "        \n",
    "train = train[good_cols]\n",
    "good_cols.remove('收率')\n",
    "test  = test[good_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并数据集\n",
    "target = train['收率']\n",
    "del train['收率']\n",
    "data = pd.concat([train,test],axis=0,ignore_index=True)\n",
    "data = data.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def timeTranSecond(t):\n",
    "#     try:\n",
    "#         t,m,s=t.split(\":\")\n",
    "#     except:\n",
    "#         if t=='1900/1/9 7:00':\n",
    "#             return 7*3600/3600\n",
    "#         elif t=='1900/1/1 2:30':\n",
    "#             return (2*3600+30*60)/3600\n",
    "#         elif t==-1:\n",
    "#             return -1\n",
    "#         else:\n",
    "#             return 0\n",
    "    \n",
    "#     try:\n",
    "#         tm = (int(t)*3600+int(m)*60+int(s))/3600\n",
    "#     except:\n",
    "#         return (30*60)/3600\n",
    "    \n",
    "#     return tm\n",
    "# for f in ['A5','A7','A9','A11','A14','A16','A24','A26','B5','B7']:\n",
    "#     try:\n",
    "#         data[f] = data[f].apply(timeTranSecond)\n",
    "#     except:\n",
    "#         print(f,'应该在前面被删除了！')\n",
    "\n",
    "# def getDuration(se):\n",
    "#     try:\n",
    "#         sh,sm,eh,em=re.findall(r\"\\d+\\.?\\d*\",se)\n",
    "#     except:\n",
    "#         if se == -1:\n",
    "#             return -1 \n",
    "        \n",
    "#     try:\n",
    "#         if int(sh)>int(eh):\n",
    "#             tm = (int(eh)*3600+int(em)*60-int(sm)*60-int(sh)*3600)/3600 + 24\n",
    "#         else:\n",
    "#             tm = (int(eh)*3600+int(em)*60-int(sm)*60-int(sh)*3600)/3600\n",
    "#     except:\n",
    "#         if se=='19:-20:05':\n",
    "#             return 1\n",
    "#         elif se=='15:00-1600':\n",
    "#             return 1\n",
    "    \n",
    "#     return tm\n",
    "# for f in ['A20','A28','B4','B9','B10','B11']:\n",
    "#     data[f] = data.apply(lambda df: getDuration(df[f]), axis=1)\n",
    "#将时间信息转化为小时\n",
    "import re\n",
    "def t2s(t):\n",
    "    try:\n",
    "        t,m,s=t.split(\":\")\n",
    "    except:\n",
    "        if t=='1900/1/9 7:00':\n",
    "            return 7\n",
    "        elif t=='1900/1/1 2:30':\n",
    "            return -1\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    try:\n",
    "        tm = int(t)*3600+int(m)*60+int(s)\n",
    "\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "    return tm/3600\n",
    "for f in ['A5','A9','A11','A14','A16','A24','A26','B5','B7']:\n",
    "    data[f] = data[f].apply(t2s)\n",
    "\n",
    "def getStartTime(se):\n",
    "\n",
    "    try:\n",
    "        sh,sm,eh,em=re.split(\"[:,-]\",se)\n",
    "    except:\n",
    "        if se=='14::30-15:30':\n",
    "            return 14.5\n",
    "        elif se=='13；00-14:00':\n",
    "            return 13\n",
    "        elif se=='21:00-22；00':\n",
    "            return 21\n",
    "        elif se=='22\"00-0:00':\n",
    "            return 22\n",
    "        elif se=='2:00-3;00':\n",
    "            return 2\n",
    "        elif se=='1:30-3;00':\n",
    "            return 1.5\n",
    "        elif se=='15:00-1600':\n",
    "            return 15\n",
    "        elif se=='19:-20:05':\n",
    "            return 19\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    try:\n",
    "        timestart = int(sh)+int(sm)/60\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "    return int(timestart)\n",
    "\n",
    "data['st6'] = data.apply(lambda df: getStartTime(df['A20']), axis=1)\n",
    "data['st9'] = data.apply(lambda df: getStartTime(df['A28']), axis=1)\n",
    "data['st10'] = data.apply(lambda df: getStartTime(df['B4']), axis=1)\n",
    "data['st13'] = data.apply(lambda df: getStartTime(df['B9']), axis=1)\n",
    "data['st14'] = data.apply(lambda df: getStartTime(df['B10']), axis=1)\n",
    "data['st15'] = data.apply(lambda df: getStartTime(df['B11']), axis=1)\n",
    "\n",
    "# def getNum(se):\n",
    "#     try:\n",
    "#         example,num=re.split(\"[-]\",se)\n",
    "#     except:\n",
    "#         return None    \n",
    "#     return int(num)\n",
    "# data['Id'] = data.apply(lambda df: getNum(df['Id']), axis=1)\n",
    "\n",
    "\n",
    "def getDuration(se):\n",
    "    timelasting = 0\n",
    "    try:\n",
    "        sh,sm,eh,em=re.split(\"[:,-]\",se)\n",
    "    except:\n",
    "        if se=='14::30-15:30':\n",
    "            return 1\n",
    "        elif se=='13；00-14:00':\n",
    "            return 1\n",
    "        elif se=='21:00-22；00':\n",
    "            return 1\n",
    "        elif se=='22\"00-0:00':\n",
    "            return 2\n",
    "        elif se=='2:00-3;00':\n",
    "            return 1\n",
    "        elif se=='1:30-3;00':\n",
    "            return 1.5\n",
    "        elif se=='15:00-1600':\n",
    "            return 1\n",
    "        elif se=='19:-20:05':\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "        \n",
    "    try:\n",
    "        if(int(eh)>int(sh)):\n",
    "            timelasting = (int(eh)*3600+int(em)*60-int(sm)*60-int(sh)*3600)/3600\n",
    "        elif(int(eh)<int(sh)):\n",
    "            timelasting = (int(eh)*3600+int(em)*60-int(sm)*60-int(sh)*3600)/3600 + 24\n",
    "    except:\n",
    "        return -1\n",
    "    return int(timelasting)\n",
    "\n",
    "for f in ['A20','A28','B4','B9','B10','B11']:\n",
    "    data[f] = data[f].apply(getDuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['样本id'] = data['样本id'].apply(lambda x: int(x.split('_')[1]))\n",
    "\n",
    "categorical_columns = [f for f in data.columns if f not in ['样本id']]\n",
    "numerical_columns = [f for f in data.columns if f not in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有风的冬老哥，在群里无意爆出来的特征，让我提升了三个个点，当然也可以顺此继续扩展\n",
    "data['b14/a1_a3_a4_a19_b1_b12'] = data['B14']/(data['A1']+data['A3']+data['A4']+data['A19']+data['B1']+data['B12'])\n",
    "\n",
    "numerical_columns.append('b14/a1_a3_a4_a19_b1_b12')\n",
    "\n",
    "# del data['A1']\n",
    "# del data['A3']\n",
    "# del data['A4']\n",
    "# categorical_columns.remove('A1')\n",
    "# categorical_columns.remove('A3')\n",
    "# categorical_columns.remove('A4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoder\n",
    "for f in categorical_columns:\n",
    "    data[f] = data[f].map(dict(zip(data[f].unique(), range(0, data[f].nunique()))))\n",
    "train = data[:train.shape[0]]\n",
    "test  = data[train.shape[0]:]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train['target'] = target\n",
    "train['intTarget'] = pd.cut(train['target'], 5, labels=False)\n",
    "train = pd.get_dummies(train, columns=['intTarget'])\n",
    "li = ['intTarget_0.0','intTarget_1.0','intTarget_2.0','intTarget_3.0','intTarget_4.0']\n",
    "mean_columns = []\n",
    "for f1 in categorical_columns:\n",
    "    cate_rate = train[f1].value_counts(normalize=True, dropna=False).values[0]\n",
    "    if cate_rate < 0.90:\n",
    "        for f2 in li:\n",
    "            col_name = 'B14_to_'+f1+\"_\"+f2+'_mean'\n",
    "            mean_columns.append(col_name)\n",
    "            order_label = train.groupby([f1])[f2].mean()\n",
    "            train[col_name] = train['B14'].map(order_label)\n",
    "            miss_rate = train[col_name].isnull().sum() * 100 / train[col_name].shape[0]\n",
    "            if miss_rate > 0:\n",
    "                train = train.drop([col_name], axis=1)\n",
    "                mean_columns.remove(col_name)\n",
    "            else:\n",
    "                test[col_name] = test['B14'].map(order_label)\n",
    "                \n",
    "train.drop(li+['target'], axis=1, inplace=True)\n",
    "train_x = train\n",
    "test_x = test\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_x = train[mean_columns+numerical_columns].values\n",
    "# test_x = test[mean_columns+numerical_columns].values\n",
    "# # one hot\n",
    "# enc = OneHotEncoder()\n",
    "# for f in categorical_columns:\n",
    "#     enc.fit(data[f].values.reshape(-1, 1))\n",
    "#     train_x = sparse.hstack((train_x, enc.transform(train[f].values.reshape(-1, 1))), 'csr')\n",
    "#     test_x = sparse.hstack((test_x, enc.transform(test[f].values.reshape(-1, 1))), 'csr')\n",
    "# print(train_x.shape)\n",
    "# print(test_x.shape)\n",
    "# print(train_x.shape)\n",
    "# print(test_x.shape)\n",
    "# train_x = pd.DataFrame(train_x.todense())\n",
    "# test_x = pd.DataFrame(test_x.todense())\n",
    "# print(type(train_x))\n",
    "# print(type(test_x))\n",
    "# xxx = pd.concat([train_x,train_y],axis = 1)\n",
    "# print(xxx.shape)\n",
    "# print(type(xxx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_y = (target[:,0])\n",
    "train_y = target.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "# from sklearn import linear_model, svm, gaussian_process\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import numpy as np\n",
    "# x = train_x\n",
    "# y = train_y\n",
    "\n",
    "# x_scaled = preprocessing.StandardScaler().fit_transform(x)\n",
    "# y_scaled = preprocessing.StandardScaler().fit_transform(y)\n",
    "# X_train,X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clfs = {\n",
    "#         'svm':svm.SVR(), \n",
    "#         'RandomForestRegressor':RandomForestRegressor(n_estimators=400),\n",
    "#         'BayesianRidge':linear_model.BayesianRidge()\n",
    "#        }\n",
    "# for clf in clfs:\n",
    "#     try:\n",
    "#         clfs[clf].fit(X_train, y_train)\n",
    "#         y_pred = clfs[clf].predict(X_test)\n",
    "#         print(clf + \" cost:\" + str((y-y_pred)**2)/(2*len(y)))\n",
    "#     except Exception as e:\n",
    "#         print(clf + \" Error:\")\n",
    "#         print(str(e))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost(未完成)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))/(2*len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.2, random_state=7)\n",
    "# # fit model on all training data\n",
    "# model = XGBClassifier()\n",
    "# model.fit(X_train, y_train)\n",
    "# plot_importance(model)\n",
    "# pyplot.show()\n",
    "# # make predictions for test data and evaluate\n",
    "# y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_test.shape)\n",
    "# print(y_pred.shape)\n",
    "# accuracy = ((y_test-y_pred)**2).mean()/2\n",
    "# print(\"Accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds = sorted(model.feature_importances_)\n",
    "# print(thresholds)\n",
    "# for thresh in thresholds:\n",
    "#     # select features using threshold\n",
    "#     selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "#     select_X_train = selection.transform(X_train)\n",
    "#     # train model\n",
    "#     selection_model = XGBClassifier()\n",
    "#     selection_model.fit(select_X_train, y_train)\n",
    "#     # eval model\n",
    "#     select_X_test = selection.transform(X_test)\n",
    "#     y_pred = selection_model.predict(select_X_test)\n",
    "#     accuracy = ((y_test-y_pred)**2).mean()/2\n",
    "#     print(\"Accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, Y_train, Y_test = train_test_split(train_x, train_y, test_size=0.2, random_state=7)\n",
    "# lgb_train = lgb.Dataset(X_train, Y_train)\n",
    "# lgb_test = lgb.Dataset(X_test, Y_test, reference=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'num_leaves': 5,\n",
    "#     'metric': ('l1', 'l2'),\n",
    "#     'verbose': 0\n",
    "# }\n",
    "\n",
    "# evals_result = {}  # to record eval results for plotting\n",
    "\n",
    "# print('Starting training...')\n",
    "# # train\n",
    "# gbm = lgb.train(params,\n",
    "#                 lgb_train,\n",
    "#                 num_boost_round=100,\n",
    "#                 valid_sets=[lgb_train, lgb_test],\n",
    "#                 feature_name=['f' + str(i + 1) for i in range(X_train.shape[-1])],\n",
    "#                 categorical_feature=[21],\n",
    "#                 evals_result=evals_result,\n",
    "#                 verbose_eval=10)\n",
    "\n",
    "# print('Plotting metrics recorded during training...')\n",
    "# ax = lgb.plot_metric(evals_result, metric='l1')\n",
    "# plt.show()\n",
    "\n",
    "# print('Plotting feature importances...')\n",
    "# ax = lgb.plot_importance(gbm, max_num_features=10)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lgb_stack_data(params,rounds,train,col,label,test):\n",
    "    ID = []\n",
    "    train = train.reset_index(drop=True)\n",
    "    kf = KFold(n_splits=5,shuffle=False)\n",
    "    i=0\n",
    "    R2_Score = []\n",
    "    RMSE = []\n",
    "    evals_result0 = {}  # to record eval results for plotting\n",
    "    for train_index, test_index in kf.split(train_x,pd.DataFrame(train_y)):\n",
    "        print(\"Training \"+str(i+1)+' Fold')\n",
    "        X_train, X_test = train_x.iloc[train_index],train_x.iloc[test_index]\n",
    "        y_train, y_test = train_y[train_index],train_y[test_index]\n",
    "        train_lgb=lgb.Dataset(X_train[col],y_train)\n",
    "        model = lgb.train(params,\n",
    "                          train_lgb,\n",
    "                          num_boost_round=rounds)\n",
    "        pred = model.predict(X_test[col])\n",
    "        X_test['label'] = list(y_test)\n",
    "        X_test['predicted'] = pred\n",
    "        r2 = score(y_test,pred)\n",
    "        print('R2 Scored of Fold '+str(i+1)+' is '+str(r2))\n",
    "        \n",
    "  \n",
    "        if i==0:\n",
    "            Final = X_test\n",
    "        else:\n",
    "            Final = Final.append(X_test,ignore_index=True)\n",
    "        i+=1\n",
    "    lgb_train_ = lgb.Dataset(train[col],label)\n",
    "    print('Start Training')\n",
    "    model_ = lgb.train(params,\n",
    "                       lgb_train_,\n",
    "                       num_boost_round=rounds,\n",
    "                       evals_result=evals_result0)\n",
    "\n",
    "    \n",
    "#     print('Plotting metrics recorded during training...')\n",
    "#     ax = lgb.plot_metric(evals_result0, metric='l1')\n",
    "#     plt.show()\n",
    "\n",
    "    print('Plotting feature importances...')\n",
    "    ax = lgb.plot_importance(model_, max_num_features=20)\n",
    "    plt.show()\n",
    "    \n",
    "    Final_pred = model_.predict(test[col])\n",
    "\n",
    "    print('Calculating In-Bag R2 Score')\n",
    "    print(score(train_y, model.predict(train[col])))\n",
    "\n",
    "    return Final,Final_pred \n",
    "def score(predictions, targets):\n",
    "    return ((predictions - targets) ** 2).mean()/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': ('l1', 'l2'),\n",
    "            'boosting': 'gbdt',\n",
    "            'learning_rate': 0.0045 , #small learn rate, large number of iterations\n",
    "            'verbose': 0,\n",
    "            'num_iterations': 500,\n",
    "            'bagging_fraction': 0.95,\n",
    "            'bagging_freq': 1,\n",
    "            'bagging_seed': 42,\n",
    "            'feature_fraction': 0.95,\n",
    "            'feature_fraction_seed': 42,\n",
    "            'max_bin': 100,\n",
    "            'max_depth': 3,\n",
    "            'num_rounds': 800\n",
    "        }\n",
    "col = list(test.columns)\n",
    "lgb_train, lgb_test = get_lgb_stack_data(params,800,pd.DataFrame(train_x),col,pd.DataFrame(train_y),pd.DataFrame(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
